{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "59ac2217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5635139b",
   "metadata": {},
   "source": [
    "# Opening the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3e01b440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>about</th>\n",
       "      <th>education_degree</th>\n",
       "      <th>experience_section_position_title</th>\n",
       "      <th>experience_section_description</th>\n",
       "      <th>certficate_name</th>\n",
       "      <th>Profile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi guys, \\nHere is an AI Research Scientist, w...</td>\n",
       "      <td>-:-M.Tech, Instrumentation and signal processi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-:-Data Analytics with Python-:-Natural Langua...</td>\n",
       "      <td>AI Research Scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi guys, \\nHere is an AI Research Scientist, w...</td>\n",
       "      <td>-:-M.Tech, Instrumentation and signal processi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-:-Data Analytics with Python-:-Natural Langua...</td>\n",
       "      <td>AI Research Scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AI Applied Research Scientist and AI Product M...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AI Research Scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As a research scientist, I specialize in the f...</td>\n",
       "      <td>-:-Anant has verified their government ID.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AI Research Scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Machine Learning Engineer with a demonstrated ...</td>\n",
       "      <td>-:-Master's degree, Mathematics-:-Bachelor of ...</td>\n",
       "      <td>-:-AI Research Scientist-:-Machine Learning En...</td>\n",
       "      <td>-:-Skills: Data Analysis\\nData Analysis-:-1) W...</td>\n",
       "      <td>-:-Building Web Applications in Django-:-Djang...</td>\n",
       "      <td>AI Research Scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>A career with a progressive organization that ...</td>\n",
       "      <td>-:-Bachelor of Business Administration - BBA, ...</td>\n",
       "      <td>-:-Associate cybersecurity analyst-:-Business ...</td>\n",
       "      <td>-:-Skills: SIEM · Soc engineer · Mc afee siem ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Software Developer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>Highly accountable, dedicated and collaborativ...</td>\n",
       "      <td>-:-MBA, Operations Research, Operations Research</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Software Developer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>• Conducting Vulnerability Assessments Using Q...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-:-Cyber Security Analyst-:-Cyber Security Ana...</td>\n",
       "      <td>-:-nan-:-• Conducting Vulnerability Assessment...</td>\n",
       "      <td>-:-AWS Security Fundamentals-:-Microsoft 365 F...</td>\n",
       "      <td>Software Developer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Software Developer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Software Developer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>505 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 about  \\\n",
       "0    Hi guys, \\nHere is an AI Research Scientist, w...   \n",
       "1    Hi guys, \\nHere is an AI Research Scientist, w...   \n",
       "2    AI Applied Research Scientist and AI Product M...   \n",
       "3    As a research scientist, I specialize in the f...   \n",
       "4    Machine Learning Engineer with a demonstrated ...   \n",
       "..                                                 ...   \n",
       "500  A career with a progressive organization that ...   \n",
       "501  Highly accountable, dedicated and collaborativ...   \n",
       "502  • Conducting Vulnerability Assessments Using Q...   \n",
       "503                                                NaN   \n",
       "504                                                NaN   \n",
       "\n",
       "                                      education_degree  \\\n",
       "0    -:-M.Tech, Instrumentation and signal processi...   \n",
       "1    -:-M.Tech, Instrumentation and signal processi...   \n",
       "2                                                  NaN   \n",
       "3           -:-Anant has verified their government ID.   \n",
       "4    -:-Master's degree, Mathematics-:-Bachelor of ...   \n",
       "..                                                 ...   \n",
       "500  -:-Bachelor of Business Administration - BBA, ...   \n",
       "501   -:-MBA, Operations Research, Operations Research   \n",
       "502                                                NaN   \n",
       "503                                                NaN   \n",
       "504                                                NaN   \n",
       "\n",
       "                     experience_section_position_title  \\\n",
       "0                                                  NaN   \n",
       "1                                                  NaN   \n",
       "2                                                  NaN   \n",
       "3                                                  NaN   \n",
       "4    -:-AI Research Scientist-:-Machine Learning En...   \n",
       "..                                                 ...   \n",
       "500  -:-Associate cybersecurity analyst-:-Business ...   \n",
       "501                                                NaN   \n",
       "502  -:-Cyber Security Analyst-:-Cyber Security Ana...   \n",
       "503                                                NaN   \n",
       "504                                                NaN   \n",
       "\n",
       "                        experience_section_description  \\\n",
       "0                                                  NaN   \n",
       "1                                                  NaN   \n",
       "2                                                  NaN   \n",
       "3                                                  NaN   \n",
       "4    -:-Skills: Data Analysis\\nData Analysis-:-1) W...   \n",
       "..                                                 ...   \n",
       "500  -:-Skills: SIEM · Soc engineer · Mc afee siem ...   \n",
       "501                                                NaN   \n",
       "502  -:-nan-:-• Conducting Vulnerability Assessment...   \n",
       "503                                                NaN   \n",
       "504                                                NaN   \n",
       "\n",
       "                                       certficate_name                Profile  \n",
       "0    -:-Data Analytics with Python-:-Natural Langua...  AI Research Scientist  \n",
       "1    -:-Data Analytics with Python-:-Natural Langua...  AI Research Scientist  \n",
       "2                                                  NaN  AI Research Scientist  \n",
       "3                                                  NaN  AI Research Scientist  \n",
       "4    -:-Building Web Applications in Django-:-Djang...  AI Research Scientist  \n",
       "..                                                 ...                    ...  \n",
       "500                                                NaN     Software Developer  \n",
       "501                                                NaN     Software Developer  \n",
       "502  -:-AWS Security Fundamentals-:-Microsoft 365 F...     Software Developer  \n",
       "503                                                NaN     Software Developer  \n",
       "504                                                NaN     Software Developer  \n",
       "\n",
       "[505 rows x 6 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Replace 'your_file_path' with the actual file paths where the Excel files are located\n",
    "file_paths = ['AI Research Scientist.xlsx','Data Scientist.xlsx',\n",
    "              'Computer Vision Engineer.xlsx', 'Natural Language Processing Engineer.xlsx',\n",
    "              'Software Developer.xlsx']\n",
    "\n",
    "# Load the data from each Excel file and assign labels\n",
    "profiles_list = []\n",
    "for path in file_paths:\n",
    "    df = pd.read_excel(path)\n",
    "    df['Profile'] = path.split('/')[-1].split('.')[0]  # Extract the profile name from the file name\n",
    "    profiles_list.append(df)\n",
    "\n",
    "# Combine all profiles into a single DataFrame\n",
    "all_profiles_df = pd.concat(profiles_list, ignore_index=True)\n",
    "\n",
    "\n",
    "columns_to_remove = ['name', 'linkedin_link', 'currentJobTitle', 'education_from_date', 'education_to_date','education_description','education_institution_name','experience_section_from_date', 'experience_section_to_date','experience_section_duration', 'experience_section_company','experience_section_location','certificate_issue_authority','certificate_issue_date', 'certificate_skill']  # Replace with your actual column names\n",
    "all_profiles_df = all_profiles_df.drop(columns_to_remove, axis=1)\n",
    "\n",
    "\n",
    "all_profiles_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b835f08e",
   "metadata": {},
   "source": [
    "# Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "898dd505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "\n",
    "# Initialize stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    # Convert text to lowercase\n",
    "    text = str(text).lower()\n",
    "    # Replace punctuation with a single space\n",
    "    text = re.sub('['+re.escape(string.punctuation)+']', ' ', text)\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub('\\s+', ' ', text).strip()\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove stopwords and lemmatize the words\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    # Join the cleaned words in a string\n",
    "    text = ' '.join(words)\n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the 'About' section\n",
    "all_profiles_df['about_cleaned'] = all_profiles_df['about'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a2f376",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0cf2a331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to categorize degree types\n",
    "def categorize_degree(degree_str):\n",
    "    if pd.isna(degree_str):\n",
    "        return {\n",
    "            'Has_PhD': 0,\n",
    "            'Has_Masters': 0,\n",
    "            'Has_Bachelors': 0,\n",
    "            'Has_Associate': 0,\n",
    "            'Has_Diploma': 0,\n",
    "        }\n",
    "    \n",
    "    # Normalize the text to make keyword matching more consistent\n",
    "    degree_str = degree_str.lower()\n",
    "    \n",
    "    return {\n",
    "        'Has_PhD': int('ph.d' in degree_str or 'phd' in degree_str or 'doctor' in degree_str),\n",
    "        'Has_Masters': int('master' in degree_str or 'mtech' in degree_str or 'm.sc' in degree_str or 'mba' in degree_str),\n",
    "        'Has_Bachelors': int('bachelor' in degree_str or 'btech' in degree_str or 'be' in degree_str or 'b.sc' in degree_str),\n",
    "        'Has_Associate': int('associate' in degree_str),\n",
    "        'Has_Diploma': int('diploma' in degree_str or 'post graduate diploma' in degree_str),\n",
    "        \n",
    "    }\n",
    "\n",
    "# Apply the categorization function to each row in the 'education_degree' column\n",
    "degree_categories = all_profiles_df['education_degree'].apply(categorize_degree)\n",
    "\n",
    "# Convert the resulting series of dictionaries to a dataframe\n",
    "degree_categories_df = pd.DataFrame(degree_categories.tolist())\n",
    "\n",
    "# Concatenate the new degree categories dataframe with the original data\n",
    "all_profiles_df = pd.concat([all_profiles_df, degree_categories_df], axis=1)\n",
    "\n",
    "\n",
    "all_profiles_df = all_profiles_df.drop(['education_degree'],axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8904c4",
   "metadata": {},
   "source": [
    "## Formating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "741705e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Define a function to split the position titles and clean them\n",
    "def extract_positions(positions_str):\n",
    "    if pd.isna(positions_str):\n",
    "        return []\n",
    "    \n",
    "    # Split the string by '-:-', trim whitespace, and filter out any empty strings\n",
    "    positions = [position.strip() for position in positions_str.split('-:-') if position.strip()]\n",
    "    return positions\n",
    "\n",
    "# Apply the function to the 'experience_section_position_title' column\n",
    "all_profiles_df['experience_positions'] = all_profiles_df['experience_section_position_title'].apply(extract_positions)\n",
    "\n",
    "# Now, 'all_profiles_df' contains a new column 'experience_positions' with the list of positions\n",
    "\n",
    "\n",
    "# Function to concatenate positions into a single standardized string\n",
    "def standardize_positions(positions_list):\n",
    "    # Join the list into a single string with spaces\n",
    "    positions_str = ' '.join(positions_list)\n",
    "    \n",
    "    # Replace newline characters with spaces\n",
    "    positions_str = positions_str.replace('\\n', ' ')\n",
    "    \n",
    "    # Remove punctuation using a regular expression\n",
    "    positions_str = re.sub(f'[{re.escape(string.punctuation)}]', '', positions_str)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    positions_str = positions_str.lower()\n",
    "    \n",
    "    return positions_str\n",
    "\n",
    "# Apply the function to concatenate and standardize the position titles\n",
    "all_profiles_df['standardized_positions'] = all_profiles_df['experience_positions'].apply(standardize_positions)\n",
    "\n",
    "columns_to_remove = ['experience_positions', 'experience_section_position_title']  # Replace with your actual column names\n",
    "all_profiles_df = all_profiles_df.drop(columns_to_remove, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d324832",
   "metadata": {},
   "source": [
    "## cleaning experience section description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "91ddcbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/amolharsh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/amolharsh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/amolharsh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download the required NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Set of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initializing the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to clean the text: remove punctuation, stop words, and perform lemmatization\n",
    "def clean_description(description):\n",
    "    if pd.isna(description):\n",
    "        return \"\"\n",
    "    \n",
    "    # Tokenize the description\n",
    "    words = word_tokenize(description)\n",
    "    \n",
    "    # Remove punctuation and stop words, and then lemmatize the remaining words\n",
    "    cleaned_description = [\n",
    "        lemmatizer.lemmatize(word.lower())\n",
    "        for word in words\n",
    "        if word.isalpha() and word.lower() not in stop_words\n",
    "    ]\n",
    "    \n",
    "    # Join the cleaned words back into a single string\n",
    "    return ' '.join(cleaned_description)\n",
    "\n",
    "# Apply the cleaning function to the 'experience_section_description' column\n",
    "all_profiles_df['cleaned_experience_description'] = all_profiles_df['experience_section_description'].apply(clean_description)\n",
    "\n",
    "# Show the results for the new column\n",
    "all_profiles_df[['experience_section_description', 'cleaned_experience_description']].head()\n",
    "\n",
    "#droping extra columns\n",
    "columns_to_remove = ['experience_section_description'] \n",
    "all_profiles_df = all_profiles_df.drop(columns_to_remove, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76680b3",
   "metadata": {},
   "source": [
    "## removing stop words and punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "222609e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/amolharsh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/amolharsh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/amolharsh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>certficate_name</th>\n",
       "      <th>cleaned_certificate_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-:-Data Analytics with Python-:-Natural Langua...</td>\n",
       "      <td>analytics language complete developer guide le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-:-Data Analytics with Python-:-Natural Langua...</td>\n",
       "      <td>analytics language complete developer guide le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-:-Building Web Applications in Django-:-Djang...</td>\n",
       "      <td>web application feature application technology...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     certficate_name  \\\n",
       "0  -:-Data Analytics with Python-:-Natural Langua...   \n",
       "1  -:-Data Analytics with Python-:-Natural Langua...   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4  -:-Building Web Applications in Django-:-Djang...   \n",
       "\n",
       "                            cleaned_certificate_name  \n",
       "0  analytics language complete developer guide le...  \n",
       "1  analytics language complete developer guide le...  \n",
       "2                                                     \n",
       "3                                                     \n",
       "4  web application feature application technology...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download the required NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Set of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initializing the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to clean the text: remove punctuation, stop words, and perform lemmatization\n",
    "def clean_certificate_name(name):\n",
    "    if pd.isna(name):\n",
    "        return \"\"\n",
    "    \n",
    "    # Tokenize the name\n",
    "    words = word_tokenize(name)\n",
    "    \n",
    "    # Remove punctuation and stop words, and then lemmatize the remaining words\n",
    "    cleaned_name = [\n",
    "        lemmatizer.lemmatize(word.lower())\n",
    "        for word in words\n",
    "        if word.isalpha() and word.lower() not in stop_words\n",
    "    ]\n",
    "    \n",
    "    # Join the cleaned words back into a single string\n",
    "    return ' '.join(cleaned_name)\n",
    "\n",
    "# Apply the cleaning function to the 'certficate_name' column\n",
    "all_profiles_df['cleaned_certificate_name'] = all_profiles_df['certficate_name'].apply(clean_certificate_name)\n",
    "\n",
    "# Show the results for the new column\n",
    "all_profiles_df[['certficate_name', 'cleaned_certificate_name']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91db9a69",
   "metadata": {},
   "source": [
    "## certificate name extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2bcf53d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = ['certficate_name']  # Replace with your actual column names\n",
    "all_profiles_df = all_profiles_df.drop(columns_to_remove, axis=1)\n",
    "all_profiles_df.to_excel('final_combined_profil.xlsx', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40612c59",
   "metadata": {},
   "source": [
    "# performing tf-idf (text to numerical conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3e07c071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<505x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 39411 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Assuming all_profiles_df is your dataframe\n",
    "# Fill missing text data with placeholder\n",
    "all_profiles_df['about_cleaned'].fillna('N/A', inplace=True)\n",
    "all_profiles_df['standardized_positions'].fillna('N/A', inplace=True)\n",
    "all_profiles_df['cleaned_experience_description'].fillna('N/A', inplace=True)\n",
    "all_profiles_df['cleaned_certificate_name'].fillna('N/A', inplace=True)\n",
    "\n",
    "# Function to create a string of qualifications based on binary columns\n",
    "def create_qualification_string(row):\n",
    "    qualifications = []\n",
    "    if row['Has_PhD'] == 1:\n",
    "        qualifications.append('phd')\n",
    "    if row['Has_Masters'] == 1:\n",
    "        qualifications.append('masters')\n",
    "    if row['Has_Bachelors'] == 1:\n",
    "        qualifications.append('bachelors')\n",
    "    if row['Has_Associate'] == 1:\n",
    "        qualifications.append('associate')\n",
    "    if row['Has_Diploma'] == 1:\n",
    "        qualifications.append('diploma')\n",
    "    return ' '.join(qualifications)\n",
    "\n",
    "# Apply the function to each row\n",
    "all_profiles_df['qualification_keywords'] = all_profiles_df.apply(create_qualification_string, axis=1)\n",
    "\n",
    "# Combine all text columns into a single text column for TF-IDF\n",
    "all_profiles_df['combined_text'] = all_profiles_df['about_cleaned'] + \" \" + \\\n",
    "                                   all_profiles_df['standardized_positions'] + \" \" + \\\n",
    "                                   all_profiles_df['cleaned_experience_description'] + \" \" + \\\n",
    "                                   all_profiles_df['cleaned_certificate_name'] + \" \" + \\\n",
    "                                   all_profiles_df['qualification_keywords']\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Fit and transform the combined text column with TF-IDF\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(all_profiles_df['combined_text'])\n",
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6cfc2f",
   "metadata": {},
   "source": [
    "# Dimension reduction\n",
    "Tried PCA, LDA NMF maximum accuracy with TruncatedSVD algo for dimension reduction¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5cf55270",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Choose the number of components, for example, 100\n",
    "# dimensionality reduction\n",
    "n_components = 100\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "\n",
    "# Fit and transform the TF-IDF matrix\n",
    "reduced_tfidf_matrix = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "# Convert the reduced matrix back to a DataFrame (optional)\n",
    "reduced_tfidf_df = pd.DataFrame(reduced_tfidf_matrix)\n",
    "\n",
    "final_features = reduced_tfidf_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6aacf26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing extra data stored locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bf46550c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your Excel file\n",
    "file_path = 'new_profiles_tf_id_truncated_data.xlsx'\n",
    "\n",
    "# Read the Excel file into a DataFrame\n",
    "new_reinforcement_truncated_tfid_data = pd.read_excel(file_path)\n",
    "new_final_text_data = pd.read_excel('profile_names_text_data.xlsx')\n",
    "final_features  = new_reinforcement_truncated_tfid_data\n",
    "all_profiles_df = new_final_text_data\n",
    "# Now the Excel file data is loaded into the DataFrame 'df'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "49478aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                                                     820\n",
       "combined_text    samyucktha g • samyucktha ganesapandian plaksh...\n",
       "Profile                                                       Test\n",
       "Name: 820, dtype: object"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_final_text_data.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd6884a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0374cfe7",
   "metadata": {},
   "source": [
    "## giving numeric value/labels to the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0ae095",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "02ff8a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Encode the target variable 'Profile'\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(all_profiles_df['Profile'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be724bab",
   "metadata": {},
   "source": [
    "# Clustering the dataset for insight generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "311f3f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_features, encoded_labels, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "X_train = final_features\n",
    "y_train = encoded_labels\n",
    "# Step 1 & 2: Compute the centroid of each class in the training data\n",
    "unique_labels = np.unique(y_train)\n",
    "centroids = {}\n",
    "unique_labels\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eeffde",
   "metadata": {},
   "source": [
    "## finding the centroid values for each class in the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "11866ece",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for label in unique_labels:\n",
    "\n",
    "    class_features = X_train[y_train == label]\n",
    "    centroids[label] = np.mean(class_features, axis=0)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd83df4",
   "metadata": {},
   "source": [
    "## finding cosine similarity for each test data in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f81bb3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Label: Computer Vision Engineer\n",
      "Predicted Label: Computational Biologist, Probability: 100.00%\n",
      "Predicted Label: Evolutionary Biologist, Probability: 77.01%\n",
      "Predicted Label: Genetic Engineer, Probability: 76.82%\n",
      "Predicted Label: Synthetic Biologist, Probability: 76.77%\n",
      "Predicted Label: Robotics Machine Learning Engineer, Probability: 51.11%\n",
      "Predicted Label: AI Research Scientist, Probability: 42.48%\n",
      "Predicted Label: Data Scientist, Probability: 42.30%\n",
      "Predicted Label: Data Analyst, Probability: 38.32%\n",
      "Predicted Label: Product Manager, Probability: 30.59%\n",
      "Predicted Label: Natural Language Processing Engineer, Probability: 28.13%\n",
      "Predicted Label: Computer Vision Engineer, Probability: 25.78%\n",
      "Predicted Label: Quant, Probability: 25.42%\n",
      "Predicted Label: Electromechanical engineer, Probability: 21.04%\n",
      "Predicted Label: RoboticsEngineer, Probability: 18.84%\n",
      "Predicted Label: Software Developer, Probability: 8.51%\n",
      "Predicted Label: Economist, Probability: 7.51%\n",
      "Predicted Label: Financial Analyst, Probability: 6.07%\n",
      "Predicted Label: Protocol engineer, Probability: 0.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def rank_classes_for_sample(sample, centroids, label_encoder):\n",
    "    distances = {}\n",
    "\n",
    "    # Convert sample to numpy array if it's a Series\n",
    "    if isinstance(sample, pd.Series):\n",
    "        sample = sample.to_numpy()\n",
    "\n",
    "    # Calculate Cosine distance for each centroid\n",
    "    for label, centroid in centroids.items():\n",
    "        # Convert centroid to numpy array if it's a Series\n",
    "        if isinstance(centroid, pd.Series):\n",
    "            centroid = centroid.to_numpy()\n",
    "\n",
    "        # Reshape centroid and sample for compatibility\n",
    "        centroid_reshaped = centroid.reshape(1, -1)\n",
    "        sample_reshaped = sample.reshape(1, -1)\n",
    "\n",
    "        # Calculate Cosine distance and store in dictionary\n",
    "        distance = cosine_distances(sample_reshaped, centroid_reshaped)[0][0]\n",
    "        \n",
    "        distances[label] = distance\n",
    "\n",
    "    # Normalize the distances to convert them into similarity scores\n",
    "    \n",
    "    max_distance = max(distances.values()) \n",
    "    min_distance = min(distances.values())\n",
    "#     print(\"distance: \", distances)\n",
    "    \n",
    "    # Normalizing distances to a range of [0, 1], where 1 is most similar\n",
    "    normalized_similarities = {label: (max_distance  - distance) / (max_distance - min_distance) if max_distance != min_distance else 1.0 for label, distance in distances.items()}\n",
    "\n",
    "    # Sort the similarities in descending order (higher is more similar)\n",
    "    sorted_similarities = sorted(normalized_similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Convert numerical labels back to original labels\n",
    "    labels = [label_encoder.inverse_transform([label])[0] for label, _ in sorted_similarities]\n",
    "\n",
    "    # Probabilities-like scores (higher is better)\n",
    "    probabilities = [similarity * 100 for _, similarity in sorted_similarities]\n",
    "\n",
    "    ranked_classes = [(labels[i], probabilities[i]) for i in range(len(labels))]\n",
    "    return ranked_classes\n",
    "\n",
    "# Example usage of the function\n",
    "# Assuming X_test, centroids, label_encoder, and y_test are already defined\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Predict for a single test sample\n",
    "i = 6 # Index of the test sample\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "#sample = X_test.iloc[i] if isinstance(X_test, pd.DataFrame) else X_test[i]\n",
    "sample = new_reinforcement_truncated_tfid_data.iloc[-1]\n",
    "\n",
    "\n",
    "flag = False\n",
    "for x in sample:\n",
    "    if x != 0:\n",
    "        flag = True\n",
    "    \n",
    "if (flag):\n",
    "    ranked_classes = rank_classes_for_sample(sample, centroids, label_encoder)\n",
    "else:\n",
    "    print(\"\\nFailed\")\n",
    "    print(\"\\n!!!!!All the values in the Series are 0!!!\\n\")\n",
    "\n",
    "# Accessing the true label index from y_test\n",
    "true_label_index = y_test[i] if isinstance(y_test, np.ndarray) else y_test.iloc[i]\n",
    "\n",
    "true_label = label_encoder.inverse_transform([true_label_index])[0]\n",
    "\n",
    "# Output\n",
    "print(f\"True Label: {true_label}\")\n",
    "for label, probability in ranked_classes:\n",
    "    print(f\"Predicted Label: {label}, Probability: {probability:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa70ac9",
   "metadata": {},
   "source": [
    "# New Evaluation metric: testing whether the true label lies in the top three ranking recommendation or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b867848d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-3 Accuracy: 93.90243902439023 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def predict_top_k_classes(sample, centroids, label_encoder, k):\n",
    "    ranked_classes = rank_classes_for_sample(sample, centroids, label_encoder)\n",
    "    # Return the top k classes\n",
    "    return label_encoder.transform([label[0] for label in ranked_classes[:k]])\n",
    "\n",
    "def top_k_accuracy_score(y_true, X_test, k, centroids, label_encoder):\n",
    "    correct = 0\n",
    "    for i in range(len(X_test)):\n",
    "        top_k_preds = predict_top_k_classes(X_test.iloc[i] if isinstance(X_test, pd.DataFrame) else X_test[i], centroids, label_encoder, k)\n",
    "        if y_true[i] in top_k_preds:\n",
    "            correct += 1\n",
    "    return correct / len(y_true)\n",
    "\n",
    "# Calculate Top-k Accuracy, for example k=3\n",
    "top_k_accuracy = top_k_accuracy_score(y_test, X_test, 3, centroids, label_encoder)\n",
    "print(f\"Top-3 Accuracy: {top_k_accuracy *100} %\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
